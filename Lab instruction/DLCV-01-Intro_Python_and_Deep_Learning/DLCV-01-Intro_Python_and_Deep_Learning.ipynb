{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf62cf5",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42426f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0cb97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4faedb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7cae8cb7ede4b29f4bf048e8b173a3b",
     "grade": false,
     "grade_id": "cell-718ea835bcbf71d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 01-Introduction to Python and Deep Learning\n",
    "\n",
    "Reference:\n",
    "- Coursera - Deep learning specialization\n",
    "\n",
    "This lab introduces on python and numpy, and implement them to be deep learning function.\n",
    "\n",
    "## About Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks are interactive coding environments embedded in a webpage. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook.\n",
    "\n",
    "We will often specify \"(â‰ˆ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
    "\n",
    "Remind that you can add cells, but do not delete the cells that I have created.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Set test to \"Hello World\" in the cell below to print \"Hello World\" and run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67f5ad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fea822e1cd2618747a9bd626b9ccf5a",
     "grade": false,
     "grade_id": "cell-0daf84bde1b6a982",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "test = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d68444",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaacc1615c8b8690e9b466aae9ac086d",
     "grade": true,
     "grade_id": "cell-3777ce099d3cfadb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "print(\"You write\", test)\n",
    "\n",
    "assert \"Hello\" in test, \"Wording is incorrect\"\n",
    "assert \"World\" in test, \"Wording is incorrect\"\n",
    "assert test[6] == 'W', \"sequence is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901decbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e61fe206925f1d44bd23f45f609a5c27",
     "grade": false,
     "grade_id": "cell-d0e865dc6da54222",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output**: You write Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d0140",
   "metadata": {},
   "source": [
    "## Building basic functions with numpy\n",
    "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments.\n",
    "\n",
    "### sigmoid function\n",
    "\n",
    "Sigmoid function is known as the logistic function. The equation can be written as\n",
    "\n",
    "$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The output graph is:\n",
    "\n",
    "<img src=\"img/Sigmoid.png\" title=\"Sigmoid graph\" style=\"width: 400px;\" />\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2068e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dfd4bcd146de4adb03e7dd3239bdd66",
     "grade": false,
     "grade_id": "cell-5e9c30e7eb768422",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "import math\n",
    "\n",
    "def math_sigmoid(x):\n",
    "    '''\n",
    "    Compute sigmoid of x\n",
    "    '''\n",
    "    z = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de4f14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17bb0a52ee3b45f87e0deb71080e9f89",
     "grade": true,
     "grade_id": "cell-e65430081268c117",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "print(math_sigmoid(5))\n",
    "\n",
    "assert math_sigmoid(10) > 0.9999, \"Calculate error\"\n",
    "assert math_sigmoid(-10) < 0.0001, \"Calculate error\"\n",
    "assert math_sigmoid(0) == 0.5, \"Calculate error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6c0b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e97eabe5d6870830ca0f0eb1b3a49c00",
     "grade": false,
     "grade_id": "cell-f7a47299b3035bdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect output**: 0.9933071490757153"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242d129",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1b0c0c96053decea5e215061628b570",
     "grade": false,
     "grade_id": "cell-cc2002a61e3ca518",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b971b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e03a6aaa501b4e3e13dc6a6d6f022fd5",
     "grade": false,
     "grade_id": "cell-260000a3cd030e7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a7da4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20abbb131089e597227be8481ced2978",
     "grade": false,
     "grade_id": "cell-b73d32c534d2fa16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In fact, if $x=(x_1,x_2,\\dots,x_n)$ is a row vector then  will apply the exponential function to every element of $x$. The output will thus be:\n",
    "\n",
    "$$np.exp(x)=(e^{x_1},e^{x_2},\\dots,e^{x_n})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed012f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cebe427ac76e2d0566e8c0febb7dd95",
     "grade": false,
     "grade_id": "cell-035383de40532aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae691801",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1388e64044f54a6f93009c7e45734f6f",
     "grade": false,
     "grade_id": "cell-a777887b5003d6b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Furthermore, if $x$ is a vector, then a Python operation such as $a=b+5$ or s=\\frac{1}{x} will output s as a vector of the same size as x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a2a66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5762c6c9836f5ec384b93bcaf2e75b99",
     "grade": false,
     "grade_id": "cell-f156a87147b76597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Any time you need more info on a numpy function, we encourage you to look at the [document](www.numpy.org).\n",
    "\n",
    "You can also create a new cell in the notebook and write np.exp? (for example) to get quick access to the documentation.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the sigmoid function using numpy.\n",
    "\n",
    "**Hint**: $x$ could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cfc3a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1c6e75a71637d4f80a3ec094f3ae79b",
     "grade": false,
     "grade_id": "cell-6092ea2aa99989d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "import numpy as np # you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    \"\"\"\n",
    "    s = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b448082",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afd4788f5ff55ae82f55b2249a226e3d",
     "grade": true,
     "grade_id": "cell-360a79e6494ceadf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "print(sigmoid(x))\n",
    "\n",
    "assert sigmoid(x).shape[0] == 3, \"Output is incorrect\" \n",
    "assert sigmoid(np.array([1,2,3,4])).shape[0] == 4, \"Output is incorrect\"\n",
    "assert sigmoid(100) > 0.99999, \"Calculation is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9a2ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4391f829a633879dc8ecb6c003dbd33",
     "grade": false,
     "grade_id": "cell-4dbffe4647a26324",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect output** : [0.73105858 0.88079708 0.95257413]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add569fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eefea6af8579611ad628023a509bc7de",
     "grade": false,
     "grade_id": "cell-7d7f6f816ee94713",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sigmoid gradient\n",
    "\n",
    "Next, let's compute sigmoid gradient which use for optimizing loss functions. The sigmoid gradient function can be calculated as\n",
    "\n",
    "$$\\nabla \\sigma(x)= \\sigma'(x) = \\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "### Exercise\n",
    "Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. \n",
    "\n",
    "**Hint**:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f3f86",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db5a3e1c26538f4f26a1b349f7a56b3d",
     "grade": false,
     "grade_id": "cell-37cc7c98709962c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \"\"\"\n",
    "    ds = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e899b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e794bcea1682eedee8bbb86ccbc3199",
     "grade": true,
     "grade_id": "cell-c9e3f9458cd93cdc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_gradient(x) = \" + str(sigmoid_grad(x)))\n",
    "\n",
    "assert sigmoid_grad(x).shape[0] == 3, \"Output shape is incorrect\"\n",
    "assert sigmoid_grad(np.array([1,2,3,4])).shape[0] == 4, \"Output is incorrect\"\n",
    "assert sigmoid_grad(2) > 0.1, \"Gradient calculation is incorrect\"\n",
    "assert sigmoid_grad(30) < 0.000000000001, \"Gradient calculation is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d41882",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed6e0dd3bb8c11fb47ff344be94a783b",
     "grade": false,
     "grade_id": "cell-850caa6f4a79d1c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect Output**: sigmoid_gradient(x) = [0.19661193 0.10499359 0.04517666]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b2328",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd51eadbcb61c62f2a2dc9083679c4fc",
     "grade": false,
     "grade_id": "cell-c03990eb0f4a5eca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Reshaping arrays\n",
    "\n",
    "Two common numpy functions used in deep learning are np.shape and np.reshape().\n",
    "\n",
    "- <code>X.shape</code> is used to get the shape (dimension) of a matrix/vector $X$.\n",
    "- <code>X.reshape(...)</code> is used to reshape $X$ into some other dimension.\n",
    "\n",
    "In computer science, an image is represented by a 3D array of shape (length, height, depth). However, when you read an image as the input of an algorithm you convert it to a vector of shape (length*height*depth, 1).\n",
    "\n",
    "<img src=\"img/image2vector_kiank.png\" title=\"image2vector_kiank\" style=\"width: 600px;\" />\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1).\n",
    "\n",
    "*Do not use hardcode the dimensions of image as a constant..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa7f30",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ea5c7594473c3c3dd065f217dd9797e",
     "grade": false,
     "grade_id": "cell-9e202a942c7b6a58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Convert image with 3 dimensions to become vector of (size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    v = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f448acf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e8feac9965c345a0269fd32af04f517",
     "grade": true,
     "grade_id": "cell-08e8ef658ee8ef4b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))\n",
    "\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"lena.png\")\n",
    "\n",
    "print(img.shape)\n",
    "vecimg = image2vector(img)\n",
    "print(vecimg.shape)\n",
    "\n",
    "assert image2vector(image).shape == (image.shape[0] * image.shape[1] * image.shape[2], 1), \"Dimension is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064af60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "254e876ce940ac983b9b697d03f6c815",
     "grade": false,
     "grade_id": "cell-00258ca0affb40a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\n",
    "image2vector(image) = [[0.67826139] \\\n",
    " [0.29380381] \\\n",
    " [0.90714982] \\\n",
    " [0.52835647] \\\n",
    " [0.4215251 ]\\\n",
    " [0.45017551]\\\n",
    " [0.92814219]\\\n",
    " [0.96677647]\\\n",
    " [0.85304703]\\\n",
    " [0.52351845]\\\n",
    " [0.19981397]\\\n",
    " [0.27417313]\\\n",
    " [0.60659855]\\\n",
    " [0.00533165]\\\n",
    " [0.10820313]\\\n",
    " [0.49978937]\\\n",
    " [0.34144279]\\\n",
    " [0.94630077]]\\\n",
    "(512, 512, 3)\\\n",
    "(786432, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ce467",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15cd9273c8482e28da8a97660e3b8fdf",
     "grade": false,
     "grade_id": "cell-7fe5f7121a3b528a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Normalizing rows\n",
    "\n",
    "Normalization is a technique for Machine Learning and Deep Learning. The technique control number parameters not overflow. It often leads to a better performance because gradient descent converges faster after normalization. For example, we have a matrix $A$\n",
    "\n",
    "$$A=\\begin{bmatrix}8&1&2 \\\\ 3&9&5 \\end{bmatrix}$$\n",
    "\n",
    "We can find normalize matrix using <code>np.linalg.norm()</code> function. In this case, we normalize with row, then\n",
    "\n",
    "$$||A|| = np.linalg.norm(A,axis=1,keepdim=True)$$\n",
    "\n",
    "and normalize the matrix by\n",
    "\n",
    "$$norm(A) = \\frac{A}{||A||}$$\n",
    "\n",
    "\n",
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement <code>normalizeRows()</code> to normalize the rows of a matrix. After applying this function to an input matrix $x$, each row of $x$ should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45a256",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c935fe429c9f89e35af6b652a04a245",
     "grade": false,
     "grade_id": "cell-c86746f3f1110396",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_x = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebab2be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14fd41481640c8349037024f59c3f914",
     "grade": true,
     "grade_id": "cell-79df5879d79d329c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "A = np.array([\n",
    "    [8, 1, 2],\n",
    "    [3, 9, 5]])\n",
    "\n",
    "norm_A = normalizeRows(A)\n",
    "print(\"normalizeRows(A) = \" + str(norm_A))\n",
    "\n",
    "sqr_A = norm_A * norm_A\n",
    "sum_A = np.sum(sqr_A, axis = 1)\n",
    "print(\"prove of A in each row\", sum_A)\n",
    "\n",
    "assert sum_A.shape == (2,), \"normalize must do in row\" \n",
    "assert np.round(sum_A[0], 1) == 1 and np.round(sum_A[1], 1) == 1, \"Normalize incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a09ac",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "normalizeRows(A) = [[0.96308682 0.12038585 0.24077171]\\\n",
    " [0.27975144 0.83925433 0.4662524 ]]\\\n",
    "prove of A in each row [1. 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d4966",
   "metadata": {},
   "source": [
    "## Broadcasting and the softmax function\n",
    "\n",
    "Broadcasting is a very important concept in numpy. It performs mathematical operations between arrays of different shapes.\n",
    "\n",
    "Softmax function or normalized exponential function is used for converting a vector into a probability distribution. The equation is\n",
    "\n",
    "$$softmax(x) = softmax([x_1,x_2,\\cdots,x_n])=\n",
    "\\begin{bmatrix}\n",
    "\\frac{e^{x_1}}{\\sum_j e^{x_j}} & \\frac{e^{x_2}}{\\sum_j e^{x_j}} & \\cdots & \\frac{e^{x_n}}{\\sum_j e^{x_j}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The softmax must implement each row independently.\n",
    "\n",
    "### Exercise\n",
    "Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a57d62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1594b6c5ec431906cc49a3faf1e36d3",
     "grade": false,
     "grade_id": "cell-3b1ced71df0e8086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax for each row of the input x.\n",
    "\n",
    "    The code should work for a row vector and also for matrices of shape (m,n).\n",
    "    \"\"\"\n",
    "    s = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d8f5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8264c75810d0ea4134d19cb3c87ffca9",
     "grade": true,
     "grade_id": "cell-3898f5f4d6361463",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x = np.array([\n",
    "    [8, 1, 3, -2, 0],\n",
    "    [7, 5, 4, 1 ,0]])\n",
    "\n",
    "s = softmax(x)\n",
    "print(\"softmax(x) = \" + str(softmax(x)))\n",
    "\n",
    "sum_s = np.sum(s, axis = 1)\n",
    "print(sum_s)\n",
    "\n",
    "assert s.shape == (2, 5), \"Softmax is incorrect\"\n",
    "assert np.round(sum_s[0]) == 1 and np.round(sum_s[1]) == 1, \"Softmax summation output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ee124",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8af833309802be2a51f17269d128c4f8",
     "grade": false,
     "grade_id": "cell-4d1fa26d077ace4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\n",
    "softmax(x) = [[9.92033287e-01 9.04617263e-04 6.68426771e-03 4.50382415e-05\\\n",
    "  3.32790093e-04]\\\n",
    " [8.41387525e-01 1.13869419e-01 4.18902183e-02 2.08559116e-03\\\n",
    "  7.67246110e-04]]\\\n",
    "[1. 1.]\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf6c1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feed274f3e1181a9ca1abed27932a769",
     "grade": false,
     "grade_id": "cell-cd9e179cfde667e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "In deep learning, you need to deal with very large datasets. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63893f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ccb4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4999e454a779e739c7d28a2c750ec7e0",
     "grade": false,
     "grade_id": "cell-f7788747196ee0bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a778ef9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90facf124fa78a4ef311731054ab2cec",
     "grade": false,
     "grade_id": "cell-75492f88aa95f86b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implement the L1 and L2 loss functions\n",
    "\n",
    "The loss L1 and L2 are used to evaluate the performance of your model. The bigger your loss is, the more different your predictions $\\hat{h}$ are from the true values $y$. In deep learning, Gradient Descent or Ascent is used to optimize models by minimizing the cost.\n",
    "\n",
    "To assume loss function in $L_1$, the L1 loss is defined as\n",
    "\n",
    "$$L_1(\\hat{y},y)=\\sum_{i=0}^m |y^{(i)}-\\hat{y}^{(i)}|$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the numpy vectorized version of the L1 loss. use function np.abs() to apply the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18433003",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61514e754a09081285972047b91ef1c0",
     "grade": false,
     "grade_id": "cell-6deb8a35949e406d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def L1(yhat, y):\n",
    "    loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed97b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33949c1051118b4d64056648bc93d8e1",
     "grade": true,
     "grade_id": "cell-4fe9b76ad5ee78cd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "yhat2 = np.array([.1, 0.7, 0.4, 0.7, .8, 0.2])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "y2 = np.array([1, 0, 0, 1, 1, 0])\n",
    "l1 = L1(yhat,y)\n",
    "l2 = L1(yhat2,y2)\n",
    "print(\"L1 of output 1 = \" + str(l1))\n",
    "print(\"L1 of output 2 = \" + str(l2))\n",
    "\n",
    "assert np.round(l1,1) == 1.1, \"L1 loss is incorrect\" \n",
    "assert np.round(l2,1) == 2.7, \"L1 loss is incorrect\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaadf69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b432204d9bd09d1fac9674cd45381ac",
     "grade": false,
     "grade_id": "cell-9f50e7f2291a0c12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\\\n",
    "L1 of output 1 = 1.1 \\\n",
    "L1 of output 2 = 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8aac4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "553f67a337137379187a9b23823645b3",
     "grade": false,
     "grade_id": "cell-f1e7466a85c7cee1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To assume loss function in $L_2$, the L2 loss is defined as\n",
    "\n",
    "$$L_1(\\hat{y},y)=\\sum_{i=0}^m (y^{(i)}-\\hat{y}^{(i)})^2$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the numpy vectorized version of the L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c95f8c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b771baa5ae1b0de3d4e8d4c810d9e242",
     "grade": false,
     "grade_id": "cell-7986f4c398c1845a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def L2(yhat, y):\n",
    "    loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8d06a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30b4cae1c54a9a82aca1ad460a32faf1",
     "grade": true,
     "grade_id": "cell-411122447a178373",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "yhat2 = np.array([.1, 0.7, 0.4, 0.7, .8, 0.2])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "y2 = np.array([1, 0, 0, 1, 1, 0])\n",
    "l1 = L2(yhat,y)\n",
    "l2 = L2(yhat2,y2)\n",
    "print(\"L2 of output 1 = \" + str(l1))\n",
    "print(\"L2 of output 2 = \" + str(l2))\n",
    "\n",
    "assert np.round(l1,2) == 0.43, \"L2 loss is incorrect\" \n",
    "assert np.round(l2,2) == 1.63, \"L2 loss is incorrect\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c200bc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4ab680821aff746cd9ff61acbacc398",
     "grade": false,
     "grade_id": "cell-c78e339c5ee218a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Classification with a Neural Network from scratch\n",
    "\n",
    "Now, let create a neural network to recognize hand writing text number.\n",
    "\n",
    "First of all, import all necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e241d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c98e4fdecd310fcdc7fe63a2a8e7129",
     "grade": false,
     "grade_id": "cell-9ab44e333cab9a01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920a809",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce612b72fa111e6199696c4acbf39418",
     "grade": false,
     "grade_id": "cell-bd028275d3fbb574",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, load the dataset. and see the data and shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cb200",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae546611638a95673164cbe04137c86a",
     "grade": false,
     "grade_id": "cell-cae0b9fedc7f39db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_digits()\n",
    "\n",
    "y_indices = data.target\n",
    "X = np.matrix(data.data)\n",
    "\n",
    "print('y shape:', y_indices.shape)\n",
    "print('X shape:', X.shape)\n",
    "\n",
    "print('y data:', y_indices[0:15])\n",
    "print('X data:', X[0:5])\n",
    "\n",
    "data_size = X.shape[0]\n",
    "x_area = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410215f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfa72743743eda8ec921513da3f7cf9b",
     "grade": false,
     "grade_id": "cell-f4cf59553daa5ab3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Show the data of X into image. Because the X data for each image is 1D vector.\n",
    "You need to convert X to image size 8x8\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create the function to convert the X data of one image to be image size 8x8. For the good function, you should check the size of vector to convert image.\n",
    "\n",
    "**Hint**: use np.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4832f4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "450bf0be8a4f52b0b9322519ef069489",
     "grade": false,
     "grade_id": "cell-c7ed53f8f4f89261",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def convert_image(X_one_image):\n",
    "    img = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67459cc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "496c837186379037d8eb479f8abd18b5",
     "grade": true,
     "grade_id": "cell-3a8545c716e68a92",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "img_0 = convert_image(X[0])\n",
    "plt.imshow(img_0, 'gray')\n",
    "plt.title('Example MNIST sample (category %d)' % y_indices[0])\n",
    "plt.show()\n",
    "\n",
    "img_5 = convert_image(X[5,:])\n",
    "plt.imshow(img_5, 'gray')\n",
    "plt.title('Example MNIST sample (category %d)' % y_indices[5])\n",
    "plt.show()\n",
    "\n",
    "test_v = np.empty([1,256])\n",
    "test = convert_image(test_v)\n",
    "\n",
    "assert img_0.shape == (8,8) and img_5.shape == (8,8), 'Image reshape is incorrect'\n",
    "assert test.shape == (16,16), 'Image reshape is incorrect'\n",
    "assert img_0[3,6] == X[0, 30] and img_5[4,2] == X[5, 34], 'Image reshape is incorrect'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e2ad1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1af0ae8091c3d51bf27c62ed113e96",
     "grade": false,
     "grade_id": "cell-936ff224b68bb94c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<img src=\"img/1expect.png\" title=\"Expect value 0\" style=\"width: 200px;\" />\n",
    "<img src=\"img/2expect.png\" title=\"Expect value 5\" style=\"width: 200px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74b20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ea3b8351dfb45578e782034e4458ada",
     "grade": false,
     "grade_id": "cell-8818bb402ac277c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## One hot encoding\n",
    "\n",
    "As you can see, the y output is index value. To use the value for classify in deep learning, you need to convert it to one hot. \n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "In this time, you need to convert the index value to be\n",
    "\n",
    "$$0 \\rightarrow [1, 0,0,0,0,0,0,0,0,0]$$\n",
    "$$1  \\rightarrow  [0, 1,0,0,0,0,0,0,0,0]$$\n",
    "$$2  \\rightarrow  [0, 0,1,0,0,0,0,0,0,0]$$\n",
    "$$3  \\rightarrow  [0, 0,0,1,0,0,0,0,0,0]$$\n",
    "$$4  \\rightarrow  [0, 0,0,0,1,0,0,0,0,0]$$\n",
    "$$5  \\rightarrow  [0, 0,0,0,0,1,0,0,0,0]$$\n",
    "$$6  \\rightarrow  [0, 0,0,0,0,0,1,0,0,0]$$\n",
    "$$7  \\rightarrow  [0, 0,0,0,0,0,0,1,0,0]$$\n",
    "$$8  \\rightarrow  [0, 0,0,0,0,0,0,0,1,0]$$\n",
    "$$9  \\rightarrow  [0, 0,0,0,0,0,0,0,0,1]$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Do one-hot vector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60569f91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce867462f288b0e40c5a020e48c6a02f",
     "grade": false,
     "grade_id": "cell-9ba154007cfd4feb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def convert_to_one_hot(y, onehot_size):\n",
    "    y_vect = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb16e64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b17885f6979722ec0940acc3c903af6",
     "grade": true,
     "grade_id": "cell-db6c257553975f4f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "y = convert_to_one_hot(y_indices, 10)\n",
    "print(y.shape)\n",
    "print(y[3])\n",
    "assert y.shape[1] == 10 and y.shape[0] == 1797, \"One hot size is incorrect\"\n",
    "assert y[14, 8] == 0 and y[177,1] == 1, \"One hot value is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8feb4e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "203341e944795c9e28cf0e6c6a965793",
     "grade": false,
     "grade_id": "cell-269fdce632dcec56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\\\n",
    "(1797, 10)\\\n",
    "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ac378",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6268cd561de425af9a1373e5565aa4ec",
     "grade": false,
     "grade_id": "cell-cffe2f1b592d45d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Normalize input feature\n",
    "\n",
    "Now, change the input X to be normalize vector. The normalize equation is\n",
    "\n",
    "$$norm(X) = \\frac{X-\\bar{X}}{SD}$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write a normalize code. If some values are nan, please change them to be zero, using np.nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a642683",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd1386438336e5bd1873458c34c85b2a",
     "grade": false,
     "grade_id": "cell-f3ea89e73910361b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def normalize(X):\n",
    "    XX = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8cef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dd99a2967d3332fce9a127d3ee9869b",
     "grade": true,
     "grade_id": "cell-556a7cf24a70ff82",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "print(XX[0])\n",
    "\n",
    "assert XX.shape == X.shape, \"Normalize function is incorrect\"\n",
    "assert np.max(XX[0]) < 2 and np.min(XX[0]) > -2, \"Data is not normalize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9b9c9",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "[[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751 -0.84412939\\\n",
    "  -0.40972392 -0.12502292 -0.05907756 -0.62400926  0.4829745   0.75962245\\\n",
    "  -0.05842586  1.12772113  0.87958306 -0.13043338 -0.04462507  0.11144272\\\n",
    "   0.89588044 -0.86066632 -1.14964846  0.51547187  1.90596347 -0.11422184\\\n",
    "  -0.03337973  0.48648928  0.46988512 -1.49990136 -1.61406277  0.07639777\\\n",
    "   1.54181413 -0.04723238  0.          0.76465553  0.05263019 -1.44763006\\\n",
    "  -1.73666443  0.04361588  1.43955804  0.         -0.06134367  0.8105536\\\n",
    "   0.63011714 -1.12245711 -1.06623158  0.66096475  0.81845076 -0.08874162\\\n",
    "  -0.03543326  0.74211893  1.15065212 -0.86867056  0.11012973  0.53761116\\\n",
    "  -0.75743581 -0.20978513 -0.02359646 -0.29908135  0.08671869  0.20829258\\\n",
    "  -0.36677122 -1.14664746 -0.5056698  -0.19600752]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c977b98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d2707073d4d28778f86062fc47009c2",
     "grade": false,
     "grade_id": "cell-28388abd546837d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Split data\n",
    "\n",
    "In deep learning, it is necessary for split your raw data to be 3 types:\n",
    "1. Training set - Data for training and learn\n",
    "2. Validate set - Data for test the network in each epoch or loop training.\n",
    "3. Test set - Data for test the network in finalize after training. This can assume that if the network is going to use, what accuracy should be.\n",
    "\n",
    "Spliting the training, validate, and test set need to make sure that\n",
    "1. The data need to be random.\n",
    "2. The validate and test set must be in the same as training set environment, but do not the same data from training set.\n",
    "3. A lot of data of training set can make your model accurate, but need to make sure that the validate and test set cover your conditions.\n",
    "\n",
    "Normally, we should split data in percentage. However, this is not fixed. You can adjust.\n",
    "- 60% training, 20% validate, and 20% test for the data over 1 million set\n",
    "- 80% training, 10% validate, and 10% test for otherwise.\n",
    "- For the very low data (~1000 data), we could use validate and test set in the same data.\n",
    "\n",
    "However, there are some trick about spliting the data when the data is too low, but we do not talk about it in here.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Random split the train set to be 60% of data and otherwise are test set.\n",
    "\n",
    "**Hint**: use <code>np.arange</code> for set index number from 0 to data_size. Random index can do by using <code>random.shuffle()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371aa561",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea83db40d8e3900f96168aa7c3fcc08",
     "grade": false,
     "grade_id": "cell-0450ac55223b7b2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "import random\n",
    "\n",
    "percent_train = .6\n",
    "# arange index number from 0 to data_size\n",
    "idx = None\n",
    "# random shuffle idx (1 line)\n",
    "\n",
    "# calculate number of training set\n",
    "m_train = None\n",
    "# split train_idx and test_idx (uncomment these 2 lines)\n",
    "# train_idx = idx[0:m_train]\n",
    "# test_idx = idx[m_train:data_size+1]\n",
    "\n",
    "# split to X_train and X_test\n",
    "X_train = None\n",
    "X_test = None\n",
    "\n",
    "# split to y_train y_test and y_test_indices\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_test_indices = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73b676",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90128c489a60d5b88957f9c71c707d04",
     "grade": true,
     "grade_id": "cell-fd1adabbbefd3415",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "assert X_train.shape[0] == int(percent_train * data_size), \"training size is incorrect\"\n",
    "assert data_size - m_train == X_test.shape[0], \"test size is incorrect\"\n",
    "assert train_idx[0] != 0 and train_idx[25] != 25, \"training indices are not shuffled\"\n",
    "assert X_train.shape == (m_train, XX.shape[1]) and y_train.shape[0] == m_train\n",
    "assert X_test.shape == (data_size - m_train, XX.shape[1]) and y_test.shape[0] == data_size - m_train and y_test_indices.shape[0] == data_size - m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39430e98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d7d72db935df1864abd24705bf3e7f5",
     "grade": false,
     "grade_id": "cell-def314a707f965cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## General Architecture of the learning algorithm\n",
    "\n",
    "It's time to design a simple algorithm to distinguish number images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset.\n",
    "\n",
    "<img src=\"img/nn_mnist.jpeg\" title=\"mnist neural network\" style=\"width: 600px;\" />\n",
    "\n",
    "*Note*: change SoftMin to be SoftMax\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create Activation functions. You need to create 3 activation functions: ReLu, Tanh, Sigmoid, and Softmax functions.\n",
    "\n",
    "ReLu equation is written\n",
    "\n",
    "$$ReLu(x) = \\max(0,x)$$\n",
    "\n",
    "Tanh equation is written\n",
    "\n",
    "$$Tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n",
    "\n",
    "Sigmoid equation is written\n",
    "\n",
    "$$Sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "And Softmax equation is written\n",
    "\n",
    "$$Softmax(x) = softmax([x_1,x_2,\\cdots,x_n])=\n",
    "\\begin{bmatrix}\n",
    "\\frac{e^{x_1}}{\\sum_j e^{x_j}} & \\frac{e^{x_2}}{\\sum_j e^{x_j}} & \\cdots & \\frac{e^{x_n}}{\\sum_j e^{x_j}}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e5e82",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1c320661a517a43cb9df6fd1c724da8",
     "grade": false,
     "grade_id": "cell-c8966ffced2b8f8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def ReLu(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56134d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189234a649df721197b4318c1b931fd7",
     "grade": true,
     "grade_id": "cell-7c798edd3404e834",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = ReLu(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"ReLu output is incorrect\"\n",
    "assert y_hat[3] > a[3] and y_hat[3] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[4] > a[4] and y_hat[4] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[0] == a[0], \"ReLu output is incorrect\"\n",
    "assert y_hat[1] == a[1], \"ReLu output is incorrect\"\n",
    "assert y_hat[2] == a[2], \"ReLu output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5092710",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b03768fa4e5cb4fe510f75f412faca17",
     "grade": false,
     "grade_id": "cell-f84a7aa889dcbde7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**: [0.9 0.2 0.1 0.  0. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389e0b7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e750a7f92a0686f61ac1142ea0359ab8",
     "grade": false,
     "grade_id": "cell-3fcb2acad0c2eeee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Tanh(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec19e25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7e468cbcd0472548a7c9354d63eda97",
     "grade": true,
     "grade_id": "cell-0836a1c1ee4396d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Tanh(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.4869, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.9610, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.9901, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.9151, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.6347, \"Tanh output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e0b49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e44f4cb3acd2625bdacd4ab2d37a87b",
     "grade": false,
     "grade_id": "cell-1a586ee96baabca7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**: [0.48691736 0.96104298 0.99006629 0.91513696 0.63473959]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478c147",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f4e2c0fcbc447ee40c70996f3b177b1",
     "grade": false,
     "grade_id": "cell-a4aa01ed23fe67f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Sigmoid(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e29f53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "651dd19414662b489b78a2ec19ec12cd",
     "grade": true,
     "grade_id": "cell-2916d85b3fb60776",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Sigmoid(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7109, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.5498, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.5250, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.4256, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.3318, \"sigmoid output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b8a95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6847ef0cce84880f7fe3c7b84733705",
     "grade": false,
     "grade_id": "cell-b5e00415dfaf393c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**: [0.7109495  0.549834   0.52497919 0.42555748 0.33181223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfa527",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89723213ea52b05dc0a9c5faabeb04f7",
     "grade": false,
     "grade_id": "cell-95d80f9e41b8821f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax for each row of the input x.\n",
    "\n",
    "    The code should work for a row vector and also for matrices of shape (m,n).\n",
    "    \"\"\"\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c48ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e0fcd9fbd30cd52598291529baba6b",
     "grade": true,
     "grade_id": "cell-05ef68e30fd3788e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Softmax(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.4083, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.2028, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.1835, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.1230, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.0824, \"Softmax output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14aeea3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d10f6497004ed088a8d2aaf91752e510",
     "grade": false,
     "grade_id": "cell-6a8d938a01f8f1d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Initializing parameters\n",
    "\n",
    "Each layer contains weight vector $w$ and bias value $b$. You can create the values as random small number or zero. We create 3 layers as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532ecec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e79edd18a7129b9dacfb0c0005f80c6",
     "grade": false,
     "grade_id": "cell-7fe04007f465093f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h2 = 5\n",
    "h1 = 6\n",
    "W = [[], np.random.normal(0,0.1,[x_area,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "B = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "\n",
    "act_funcs = [None, ReLu, Sigmoid, Softmax]\n",
    "\n",
    "L = len(W)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e91eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a17cad16a4aa7354a275cb9b18496503",
     "grade": false,
     "grade_id": "cell-cdbb85360f100996",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Forward\n",
    "\n",
    "For input $x^{(i)}$, the forward propagation in each layer can be calculated by\n",
    "$$z^{(i)}=W^Tx^{(i)}+b$$\n",
    "$$\\hat{y}^{(i)}=a^{(i)}=act(z^{(i)})$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create forward_layer function which input self-define activation function\n",
    "\n",
    "*Note*: If input act_func as None, the output is linear activation function\n",
    "\n",
    "**Hint**: use <code>*</code> for multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00849d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bff503177f4b68dc0394f24c3eea3c8e",
     "grade": false,
     "grade_id": "cell-44c2d8c98914f9b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def forward_layer(w, b, X, act_func):\n",
    "    # z is linear function\n",
    "    z = None\n",
    "    # y_hat is output after activation function\n",
    "    y_hat = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd642c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc9e79a1c2f4bd80423d4c82fdfa050",
     "grade": true,
     "grade_id": "cell-db37cafcdd0d8f2d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([[.9, 0.2, 0.1, -0.3, -0.7]]).T\n",
    "\n",
    "w = np.array([[0.2, 0.1, 1, 3, 0.5]])\n",
    "b = np.array([[1]])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, b, X, None)\n",
    "b = np.array([[0.5]])\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, None)\n",
    "print('Linear output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1[2,0] == 1.1\n",
    "assert np.round(y_hat2[3,0], 4) == -0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b8c1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b493bc41002bf214d82ea0f691983e7c",
     "grade": true,
     "grade_id": "cell-c0a86bd1a1d677be",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, ReLu)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, ReLu)\n",
    "print('ReLu output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1[3] > 0, \"Forward layer is incorrect\"\n",
    "assert y_hat2[3] == 0, \"Forward layer is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8a52a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b6c7fea24a80fab0274e6df1626b50",
     "grade": true,
     "grade_id": "cell-a3723a732d422843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, Tanh)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, Tanh)\n",
    "print('Tanh output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1.shape[0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df5808",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e44465ef91e0e604e02d8b155ac1a3ac",
     "grade": true,
     "grade_id": "cell-24c1009e76a9f0a0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, Sigmoid)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, Sigmoid)\n",
    "print('Sigmoid output of y_hat1', y_hat1, 'and', y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663193fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fcf65674598775c50037629ec3f9390",
     "grade": true,
     "grade_id": "cell-89fa77d7b52e9903",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([[.9, 0.2, 0.1, -0.3, -0.7]]).T\n",
    "\n",
    "w = np.array([[0.2, 0.1, 1, 3, 0.5], [0.3, 0.5, 0.1, -0.3, -0.5]])\n",
    "b = np.array([-1,3])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, b, X, Softmax)\n",
    "print('Linear output of z1', z1)\n",
    "print('Softmax output of y_hat1', y_hat1)\n",
    "\n",
    "assert y_hat1.shape == (5, 2), \"Forward layer is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815d8aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d2450c0d0aa4c10c2adb75601da09f1",
     "grade": false,
     "grade_id": "cell-4f8d9aee2a0165d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**:\\\n",
    "Linear output of y_hat1 0.050000000000000155 and -0.44999999999999984\\\n",
    "ReLu output of y_hat1 0.050000000000000155 and 0.0\\\n",
    "Tanh output of y_hat1 0.9975041607715679 and 0.822001229369054\\\n",
    "Sigmoid output of y_hat1 0.5124973964842104 and 0.389360766050778\\\n",
    "Linear output of z1 [-1.95  3.82]\\\n",
    "Softmax output of y_hat1 [0.00311005 0.99688995]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747bf28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb5f5ed8c17c4cf23a9d3d4f62c7af43",
     "grade": false,
     "grade_id": "cell-f87ffc164bd24ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Create full of forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590ebba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e70fd8ad68f87f4778ea928a008970e",
     "grade": false,
     "grade_id": "cell-c2e654c5e61d922f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def forward_one_step(X, W, B, act_funcs):\n",
    "    L = len(W)-1\n",
    "    a = [X]\n",
    "    z = [[]]\n",
    "    delta = [[]]\n",
    "    dW = [[]]\n",
    "    db = [[]]\n",
    "    for l in range(1,L+1):\n",
    "        z_layer, a_layer = None, None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        z.append(z_layer)\n",
    "        a.append(a_layer)\n",
    "        # Just to give arrays the right shape for the backprop step\n",
    "        delta.append([]); dW.append([]); db.append([])\n",
    "    return a, z, delta, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a84fb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e543289e32e627566d2b57bd682055e9",
     "grade": true,
     "grade_id": "cell-043d31f1f9361217",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "\n",
    "a, z, delta, dW, db = forward_one_step(x_this, W, B, act_funcs)\n",
    "print('size of a', len(a), 'a[3] =', a[3])\n",
    "print('size of z', len(z), 'z[3] =', z[3])\n",
    "\n",
    "assert len(a) == len(z) and len(a) == 4\n",
    "assert a[0].shape == (64,1)\n",
    "assert a[1].shape == (6,1)\n",
    "assert a[2].shape == (5,1)\n",
    "assert a[3].shape == (10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b544d2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "def67cefa90e7d9f1aeff42191e7975c",
     "grade": false,
     "grade_id": "cell-0bf1778d701edd29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output** (The output may not the same):\\\n",
    "size of a 4 a[3] = [[0.09043188]\\\n",
    " [0.11294963]\\\n",
    " [0.06708026]\\\n",
    " [0.11812721]\\\n",
    " [0.11817167]\\\n",
    " [0.10483097]\\\n",
    " [0.09818055]\\\n",
    " [0.08948474]\\\n",
    " [0.10362842]\\\n",
    " [0.09711466]]\\\n",
    "size of z 4 z[3] = [[-0.05075633]\\\n",
    " [ 0.17158877]\\\n",
    " [-0.34946347]\\\n",
    " [ 0.21640886]\\\n",
    " [ 0.2167852 ]\\\n",
    " [ 0.096996  ]\\\n",
    " [ 0.03145495]\\\n",
    " [-0.06128506]\\\n",
    " [ 0.0854584 ]\\\n",
    " [ 0.02053917]]\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2cf99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "503780eb6c5523d20ea6e642987f26ac",
     "grade": false,
     "grade_id": "cell-0433badbd5f324de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "For softmax loss function, it is cross entropy loss. You can calculate as\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i=0}^n (y_i * \\log\\hat{y}_i)$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create loss function for multi classification (cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba549d6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c554d653b4aef620c12db24c02dd274a",
     "grade": false,
     "grade_id": "cell-21c14f2813642e8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def loss(y, yhat):\n",
    "    l = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c31c75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69c6d7ea9eef1bc61d11ae3eabf09404",
     "grade": true,
     "grade_id": "cell-cb8cb82134e135e7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "y_hat = np.array([0.4083291, 0.20277023, 0.18347409, 0.12298636, 0.08244022])\n",
    "y = np.array([0, 1, 0, 0, 0])\n",
    "\n",
    "l = loss(y, y_hat)\n",
    "print(l)\n",
    "\n",
    "assert np.round(l, 4) == 1.5957, \"Loss function incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0956c0ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbbe389df8a83196e0eeefb35edbec96",
     "grade": false,
     "grade_id": "cell-deb99febadc56aa1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected Output**: 1.5956818129123256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239757c",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "Back propagation can be calculated as\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial z^{[l-1]} } =[W^{[l]}]^T \\cdot \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} } * {g^{[l-1]}}'(z^{[l-1]})$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial W^{[l]} } = \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} } \\cdot [a^{[l-1]}]^T$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial b^{[l]} } = \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} }$$\n",
    "\n",
    "When ${g^{[l-1]}}'$ is derivative activation function.\n",
    "\n",
    "Thus first of all, we need to calculate derivative of the activation functions that we use.\n",
    "\n",
    "The Linear_derivative ($dl$) function is\n",
    "$$dl(x) = [1]$$\n",
    "\n",
    "The ReLu_derivative ($dReLu$) function is\n",
    "\n",
    "$$dReLu(x) = [\\text{1 when x>0, otherwise 0}]$$\n",
    "\n",
    "The Tanh_derivative ($dTanh$) function is\n",
    "\n",
    "$$dTanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "The Sigmoid_derivative ($ds$) function is\n",
    "\n",
    "$$ds(x) = sigmoid(x)(1-sigmoid(x))$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write the derivative functions as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b6443",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68a34ab52b0fb2632b57cb262d0b8459",
     "grade": false,
     "grade_id": "cell-8a8c9802ba726b02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Linear_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7017168",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d87c78399158f95d461dfec72c2f497",
     "grade": true,
     "grade_id": "cell-66bb0a8919622722",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Linear_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[0] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[1] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[2] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[3] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[4] == 1, \"Linear_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef0dee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98816488d28a5ed95c2bf9047a4233bc",
     "grade": false,
     "grade_id": "cell-93cdcfe58c82d885",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def ReLu_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191e63b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85f04d94e812d5872a7f3bd797084108",
     "grade": true,
     "grade_id": "cell-a8aa5d762da71155",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = ReLu_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[0] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[1] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[2] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[3] == 0, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[4] == 0, \"ReLu_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdde55",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c8758753be7ce30ecdfa495b1b815be",
     "grade": false,
     "grade_id": "cell-5d0fb8889a4ecb72",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Tanh_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff71ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ea1e2f6e129f91c87a6e4690c1416b7",
     "grade": true,
     "grade_id": "cell-6293332bf43d707a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Tanh_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7629, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.0764, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.0198, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.1625, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.5971, \"Tanh_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a68f4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c685413aabf3a8353cd7bad7fc0adee8",
     "grade": false,
     "grade_id": "cell-60c5ef6bb5424b3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Sigmoid_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641bbfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea7b89c68194820354eec0f20a4495ed",
     "grade": true,
     "grade_id": "cell-e263208090ae0369",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Sigmoid_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.2055, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.2475, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.2494, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.2445, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.2217, \"Sigmoid_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c9017",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d4b333b961a98a2a523230c39174e64",
     "grade": false,
     "grade_id": "cell-35cbea09813769f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Create back propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb2bc7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e0175819302f9aac76ff477459200f2",
     "grade": false,
     "grade_id": "cell-1373f85d1e10d35b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def back_propagation(y, a, z, W, dW, db, act_deri):\n",
    "    '''\n",
    "    Backprop step. Note that derivative of multinomial cross entropy\n",
    "    loss is the same as that of binary cross entropy loss. See\n",
    "    https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "    for a nice derivation.\n",
    "    '''\n",
    "    L = len(W)-1\n",
    "    \n",
    "    # y_hat - y\n",
    "    # delta[L] = None\n",
    "    for l in range(L,0,-1):\n",
    "        # db = delta(l)\n",
    "        db[l] = None\n",
    "        \n",
    "        # dW = a(l-1) * delta(l)\n",
    "        dW[l] = None\n",
    "        \n",
    "        if l > 1:\n",
    "            # recalculate delta in backward layer\n",
    "            # dAct_func(z(l-1)) * (W(l) * delta(l))\n",
    "            delta[l-1] = None\n",
    "            \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f7e4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edeeda374539b01be315873cd50f6768",
     "grade": false,
     "grade_id": "cell-8f7dc3731a0c025d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create activation derivative variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3b51f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4968632aa004ba0339a56b45a41b7fe",
     "grade": false,
     "grade_id": "cell-8fc13285edf3fea5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "act_deri = [None, ReLu_derivative, Sigmoid_derivative, Softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6592f74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a2a54d338fe8693ee16b10f56aaf13",
     "grade": true,
     "grade_id": "cell-e7807c003e439823",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "y_this = y_train[0,:]\n",
    "\n",
    "a, z, delta, dW, db = forward(x_this, W, B, act_funcs)\n",
    "dW, db = back_propagation(y_this, a, z, W, dW, db, act_deri)\n",
    "\n",
    "lenW = [0, 64, 6, 5]\n",
    "for i in range(4):\n",
    "    assert len(dW[i]) == lenW[i]\n",
    "    \n",
    "print(\"dW\", dW)\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89440320",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e8b0f03c658373eea438319d54e5ce5",
     "grade": false,
     "grade_id": "cell-22f8b815228d7772",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Update weight and bias\n",
    "\n",
    "In the training, to improve accuracy, you need to update weight/bias while training.\n",
    "Weight and bias update equations are\n",
    "$$\n",
    "W_{new}^{(i)} = W_{old}^{(i)} - \\alpha * \\delta W\n",
    "$$\n",
    "$$\n",
    "B_{new}^{(i)} = B_{old}^{(i)} - \\alpha * \\delta B\n",
    "$$\n",
    "\n",
    "When $\\alpha$ is learning rate. and $i$ is the layer number of network\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create <code>update_step</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd9c1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ffcaddb29d68b58455f9c365fb7e250",
     "grade": false,
     "grade_id": "cell-eefff648d60c5a50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def update_step(W, B, dW, db, alpha):\n",
    "    L = len(W)-1\n",
    "    for l in range(1,L+1):\n",
    "        # W[l] = None\n",
    "        # B[l] = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67989c36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9fc82c2304c7a7eb47bcc1998881cac",
     "grade": true,
     "grade_id": "cell-8ec04f0244f4bef4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "y_this = y_train[0,:]\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "a, z, delta, dW, db = forward(x_this, W, B, act_funcs)\n",
    "dW, db = back_propagation(y_this, a, z, W, dW, db, act_deri)\n",
    "\n",
    "W_new, B_new = update_step(W, B, dW, db, alpha)\n",
    "W_new_2, B_new_2 = update_step(W_new, B_new, dW, db, alpha)\n",
    "\n",
    "result_w = np.array_equal(W, W_new)\n",
    "result_w2 = np.array_equal(W_new, W_new_2)\n",
    "assert W[2].shape == W_new[2].shape and W[1].shape == W_new_2[1].shape, \"W_new shape must be the same\"\n",
    "assert not result_w and not result_w2, \"Weight must be updated\"\n",
    "\n",
    "result_b = np.array_equal(B, B_new)\n",
    "result_b2 = np.array_equal(B_new, B_new_2)\n",
    "assert B[3].shape == B_new[3].shape and B[1].shape == B_new_2[1].shape, \"b_new shape must be the same\"\n",
    "assert not result_b and not result_b2, \"Bias must be updated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca437db9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04b3fdf10b341215d1215156b267df89",
     "grade": false,
     "grade_id": "cell-6c5caa88d85af3f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Put it together\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create training code using the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4fa41",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3312de620fee1720dde00ea6c031f734",
     "grade": false,
     "grade_id": "cell-22411ad703a7bf45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "cost_arr = [] \n",
    "\n",
    "alpha = 0.01\n",
    "max_iter = 100\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    # random index of m_train\n",
    "    order = np.random.permutation(m_train)\n",
    "    for i in range(0, m_train):\n",
    "        # Grab the pattern order[i]\n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "        \n",
    "        # Feed forward step\n",
    "        a, z, delta, dW, db = None, None, None, None, None\n",
    "        # calulate loss for each epoch\n",
    "        loss_this_pattern = 0 #(calculate loss here)\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        # back propagation\n",
    "        dW, db = None, None\n",
    "        # update weight, bias (1 line)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    cost_arr.append(loss_this_iter[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32b24f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d79021ed58deca523f8766aadcfd0cc8",
     "grade": true,
     "grade_id": "cell-8f7a21a677a79a20",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "for i in range(max_iter):\n",
    "    print('Epoch %d train loss %f' % (i + 1, cost_arr[i]))\n",
    "assert len(cost_arr) == max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfdf4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d2c8916c67bf2eb065f79f15cbfa19",
     "grade": false,
     "grade_id": "cell-3977ac3287a29744",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Take home Exercise\n",
    "\n",
    "1. Plot the loss value into graph using pyplot (10 points)\n",
    "2. Create Prediction function to predict the test set which we have separated from above. Calculate the accuracy of prediction. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603eea6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d69ce88fe5c893ec0cbf42d7c7ebeac4",
     "grade": true,
     "grade_id": "cell-a54e2e1abb163dd7",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca288e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d77bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85db26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
