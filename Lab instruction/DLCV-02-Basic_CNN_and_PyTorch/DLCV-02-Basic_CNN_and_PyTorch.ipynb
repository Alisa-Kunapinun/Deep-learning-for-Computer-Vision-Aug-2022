{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70473913",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3aa7d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03abea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2570431adbf3c8bba205b7aca1dae16a",
     "grade": false,
     "grade_id": "cell-4a2a486c671edce9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 02 Basic CNN and VGG/AlexNet network\n",
    "\n",
    "Today we'll try to understand how CNNs are implemented using a library such as PyTorch and what the library elements are actually doing.\n",
    "\n",
    "## CNN Explainer\n",
    "\n",
    "Before doing this tutorial, take a look at [the CNN Explainer](https://poloclub.github.io/cnn-explainer/).\n",
    "It gives beautiful illustrations of what's happening in a CNN at every level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4ecdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08af8d151f0fe28aa32030a8c03b7c3",
     "grade": false,
     "grade_id": "cell-223d51fac614ddb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Connect to google drive for colab users\n",
    "\n",
    "For the students who use google colab for run the system. It is better to upload your dataset or weight, model into your drive. The colab system will be reset everytime when the system has log off.\n",
    "\n",
    "Ok, let's mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37563f51",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8233792faf7af546c488fc6982a6d13b",
     "grade": false,
     "grade_id": "cell-80673117de9f261c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# for colab only\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Your root path in gdrive\n",
    "root_path = 'gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5005911",
   "metadata": {},
   "source": [
    "Let's create folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1749fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for create the folder\n",
    "import os\n",
    "\n",
    "lab_path =  root_path + 'lab02/'\n",
    "if not os.path.exists(lab_path):\n",
    "  print(\"No folder \", lab_path, 'exist. Create the folder')\n",
    "  os.mkdir(lab_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbbe37b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb20e019ddd72f93f89f87e8b43a2ecc",
     "grade": false,
     "grade_id": "cell-fdc74f664d58c848",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Create your local folder\n",
    "\n",
    "For the person who want to use python in your local computer, you can create folder as below.\n",
    "\n",
    "Note that google colab can use the command below too, but it will create the drive in the virtual machine. Thus, your files/folders that you have created will be gone after the system closed. Be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f980e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_path =  'lab02/'\n",
    "if not os.path.exists(lab_path):\n",
    "  print(\"No folder \", lab_path, 'exist. Create the folder')\n",
    "  os.mkdir(lab_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9c733",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cdd33804c7147c9e64e83707176d7a9",
     "grade": false,
     "grade_id": "cell-5fdb3b05f4bb2862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the person who want to use google drive as your memory you can check [PyDrive](https://pypi.org/project/PyDrive/). In this class, we do not talk about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acc219",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9df9d4bfb537968ce0093ab4ea4ae995",
     "grade": false,
     "grade_id": "cell-ae6a089b816c9b68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Hand-coded CNN\n",
    "\n",
    "This example is based on [Ahmed Gad's tutorial](https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html).\n",
    "\n",
    "We will implement a very simple CNN in numpy. The model will have\n",
    "just three layers, a convolutional layer (conv for short), a ReLU activation layer, and max pooling. The major steps involved are as follows.\n",
    "1. Reading the input image.\n",
    "2. Preparing filters.\n",
    "3. Conv layer: Convolving each filter with the input image.\n",
    "4. ReLU layer: Applying ReLU activation function on the feature maps (output of conv layer).\n",
    "5. Max Pooling layer: Applying the pooling operation on the output of ReLU layer.\n",
    "6. Stacking the conv, ReLU, and max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5a1d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d7a6d3ad043faddf72df4841f209f4e",
     "grade": false,
     "grade_id": "cell-5a16542caa7b22e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Reading an input image\n",
    " \n",
    "The following code reads an existing image using the SciKit-Image Python library and converts it into grayscale. You may need to `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read image\n",
    "img = skimage.data.chelsea()\n",
    "print('Image dimensions:', img.shape)\n",
    "\n",
    "# Convert to grayscale\n",
    "img = skimage.color.rgb2gray(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde88b78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8de0fb2b4ae262d7aee2f7c8ab1dcfe0",
     "grade": false,
     "grade_id": "cell-8407a987acdc5a70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Create some filters for the conv layer\n",
    "\n",
    "Recall that a conv layer uses some number of convolution (actually cross correlation) filters, usually matching the number\n",
    "of channels in the input (1 in our case since the image is grayscale). Each kernel gives us one feature map (channel) in the\n",
    "result.\n",
    "\n",
    "Let's make two 3$\\times$3 filters, using the horizontal and vertical Sobel edge filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filters = np.zeros((2,3,3))\n",
    "l1_filters[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                 [-2, 0, 2], \n",
    "                                 [-1, 0, 1]]])\n",
    "l1_filters[1, :, :] = np.array([[[-1, -2, -1], \n",
    "                                 [ 0,  0,  0], \n",
    "                                 [ 1,  2,  1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0324573",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6eeed40630d85e422c1417c87fd2f75",
     "grade": false,
     "grade_id": "cell-dfc53f28d4e37a2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conv layer feedforward step\n",
    " \n",
    "Let's convolve the input image with our filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stride 1 cross correlation of an image and a filter. We output the valid region only\n",
    "# (no padding).\n",
    "def convolve(img, conv_filter):\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    results_dim = ((np.array(img.shape) - np.array(conv_filter.shape) + (2*padding))/stride) + 1\n",
    "    result = np.zeros((int(results_dim[0]), int(results_dim[1])))\n",
    "    \n",
    "    for r in np.arange(0, img.shape[0] - filter_size + 1):\n",
    "        for c in np.arange(0, img.shape[1]-filter_size + 1):          \n",
    "            curr_region = img[r:r+filter_size,c:c+filter_size]\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result)\n",
    "            result[r, c] = conv_sum\n",
    "    \n",
    "    return result       \n",
    "\n",
    "# Perform convolution with a set of filters and return the result\n",
    "def conv(img, conv_filters):\n",
    "    # Check shape of inputs\n",
    "    if len(img.shape) != len(conv_filters.shape) - 1: \n",
    "        raise Exception(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "\n",
    "    # Ensure filter depth is equal to number of channels in input\n",
    "    if len(img.shape) > 2 or len(conv_filters.shape) > 3:\n",
    "        if img.shape[-1] != conv_filters.shape[-1]:\n",
    "            raise Exception(\"Error: Number of channels in both image and filter must match.\")\n",
    "            \n",
    "    # Ensure filters are square\n",
    "    if conv_filters.shape[1] != conv_filters.shape[2]: \n",
    "        raise Exception('Error: Filter must be square (number of rows and columns must match).')\n",
    "\n",
    "    # Ensure filter dimensions are odd\n",
    "    if conv_filters.shape[1]%2==0: \n",
    "        raise Exception('Error: Filter must have an odd size (number of rows and columns must be odd).')\n",
    "\n",
    "    # Prepare output\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filters.shape[1]+1, \n",
    "                             img.shape[1]-conv_filters.shape[1]+1, \n",
    "                             conv_filters.shape[0]))\n",
    "\n",
    "    # Perform convolutions\n",
    "    for filter_num in range(conv_filters.shape[0]):\n",
    "        curr_filter = conv_filters[filter_num, :]\n",
    "        # Our convolve function only handles 2D convolutions. If the input has multiple channels, we\n",
    "        # perform the 2D convolutions for each input channel separately then add them. If the input\n",
    "        # has just a single channel, we do the convolution directly.\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = convolve(img[:, :, 0], curr_filter[:, :, 0])\n",
    "            for ch_num in range(1, curr_filter.shape[-1]):\n",
    "                conv_map = conv_map + convolve(img[:, :, ch_num], \n",
    "                                      curr_filter[:, :, ch_num])\n",
    "        else:\n",
    "            conv_map = convolve(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7a2c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ece95eb7c9e80554d70e2ecd0093c258",
     "grade": false,
     "grade_id": "cell-8787f0ab91af2634",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conv(img, l1_filters)\n",
    "%timeit conv(img,l1_filters)\n",
    "\n",
    "print('Convolutional feature maps shape:', features.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(features[:,:,0], cmap='gray')\n",
    "ax2.imshow(features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ae7e9",
   "metadata": {},
   "source": [
    "## Pooling and relu\n",
    "\n",
    "Next, consider the feedforward pooling and ReLU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer with particular size and stride\n",
    "\n",
    "def pooling(feature_map, size=2, stride=2):\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                         np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                         feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "def relu(feature_map):\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a2dc7",
   "metadata": {},
   "source": [
    "Now let's try ReLU and pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relued_features = relu(features)\n",
    "pooled_features = pooling(relued_features)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "ax1.imshow(relued_features[:,:,0], cmap='gray')\n",
    "ax2.imshow(relued_features[:,:,1], cmap='gray')\n",
    "ax3.imshow(pooled_features[:,:,0], cmap='gray')\n",
    "ax4.imshow(pooled_features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d60fc",
   "metadata": {},
   "source": [
    "Let's visualize all of the feature maps in the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First conv layer\n",
    "\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(\"conv layer 1...\")\n",
    "l1_feature_maps = conv(img, l1_filters)\n",
    "l1_feature_maps_relu = relu(l1_feature_maps)\n",
    "l1_feature_maps_relu_pool = pooling(l1_feature_maps_relu, 2, 2)\n",
    "\n",
    "# Second conv layer\n",
    "\n",
    "print(\"conv layer 2...\")\n",
    "l2_filters = np.random.rand(3, 5, 5, l1_feature_maps_relu_pool.shape[-1])\n",
    "l2_feature_maps = conv(l1_feature_maps_relu_pool, l2_filters)\n",
    "l2_feature_maps_relu = relu(l2_feature_maps)\n",
    "l2_feature_maps_relu_pool = pooling(l2_feature_maps_relu, 2, 2)\n",
    "#print(l2_feature_maps)\n",
    "\n",
    "# Third conv layer\n",
    "\n",
    "print(\"conv layer 3...\")\n",
    "l3_filters = np.random.rand(1, 7, 7, l2_feature_maps_relu_pool.shape[-1])\n",
    "l3_feature_maps = conv(l2_feature_maps_relu_pool, l3_filters)\n",
    "l3_feature_maps_relu = relu(l3_feature_maps)\n",
    "l3_feature_maps_relu_pool = pooling(l3_feature_maps_relu, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "\n",
    "fig0, ax0 = plt.subplots(nrows=1, ncols=1)\n",
    "ax0.imshow(img).set_cmap(\"gray\")\n",
    "ax0.set_title(\"Input Image\")\n",
    "ax0.get_xaxis().set_ticks([])\n",
    "ax0.get_yaxis().set_ticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "fig1, ax1 = plt.subplots(nrows=3, ncols=2)\n",
    "fig1.set_figheight(10)\n",
    "fig1.set_figwidth(10)\n",
    "ax1[0, 0].imshow(l1_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[0, 0].get_xaxis().set_ticks([])\n",
    "ax1[0, 0].get_yaxis().set_ticks([])\n",
    "ax1[0, 0].set_title(\"L1-Map1\")\n",
    "\n",
    "ax1[0, 1].imshow(l1_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[0, 1].get_xaxis().set_ticks([])\n",
    "ax1[0, 1].get_yaxis().set_ticks([])\n",
    "ax1[0, 1].set_title(\"L1-Map2\")\n",
    "\n",
    "ax1[1, 0].imshow(l1_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[1, 0].get_xaxis().set_ticks([])\n",
    "ax1[1, 0].get_yaxis().set_ticks([])\n",
    "ax1[1, 0].set_title(\"L1-Map1ReLU\")\n",
    "\n",
    "ax1[1, 1].imshow(l1_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[1, 1].get_xaxis().set_ticks([])\n",
    "ax1[1, 1].get_yaxis().set_ticks([])\n",
    "ax1[1, 1].set_title(\"L1-Map2ReLU\")\n",
    "\n",
    "ax1[2, 0].imshow(l1_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 0].set_title(\"L1-Map1ReLUPool\")\n",
    "\n",
    "ax1[2, 1].imshow(l1_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 1].set_title(\"L1-Map2ReLUPool\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae21831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "fig2, ax2 = plt.subplots(nrows=3, ncols=3)\n",
    "fig2.set_figheight(12)\n",
    "fig2.set_figwidth(12)\n",
    "ax2[0, 0].imshow(l2_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[0, 0].get_xaxis().set_ticks([])\n",
    "ax2[0, 0].get_yaxis().set_ticks([])\n",
    "ax2[0, 0].set_title(\"L2-Map1\")\n",
    "\n",
    "ax2[0, 1].imshow(l2_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[0, 1].get_xaxis().set_ticks([])\n",
    "ax2[0, 1].get_yaxis().set_ticks([])\n",
    "ax2[0, 1].set_title(\"L2-Map2\")\n",
    "\n",
    "ax2[0, 2].imshow(l2_feature_maps[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[0, 2].get_xaxis().set_ticks([])\n",
    "ax2[0, 2].get_yaxis().set_ticks([])\n",
    "ax2[0, 2].set_title(\"L2-Map3\")\n",
    "\n",
    "ax2[1, 0].imshow(l2_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[1, 0].get_xaxis().set_ticks([])\n",
    "ax2[1, 0].get_yaxis().set_ticks([])\n",
    "ax2[1, 0].set_title(\"L2-Map1ReLU\")\n",
    "\n",
    "ax2[1, 1].imshow(l2_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[1, 1].get_xaxis().set_ticks([])\n",
    "ax2[1, 1].get_yaxis().set_ticks([])\n",
    "ax2[1, 1].set_title(\"L2-Map2ReLU\")\n",
    "\n",
    "ax2[1, 2].imshow(l2_feature_maps_relu[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[1, 2].get_xaxis().set_ticks([])\n",
    "ax2[1, 2].get_yaxis().set_ticks([])\n",
    "ax2[1, 2].set_title(\"L2-Map3ReLU\")\n",
    "\n",
    "ax2[2, 0].imshow(l2_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[2, 0].get_xaxis().set_ticks([])\n",
    "ax2[2, 0].get_yaxis().set_ticks([])\n",
    "ax2[2, 0].set_title(\"L2-Map1ReLUPool\")\n",
    "\n",
    "ax2[2, 1].imshow(l2_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[2, 1].get_xaxis().set_ticks([])\n",
    "ax2[2, 1].get_yaxis().set_ticks([])\n",
    "ax2[2, 1].set_title(\"L2-Map2ReLUPool\")\n",
    "\n",
    "ax2[2, 2].imshow(l2_feature_maps_relu_pool[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[2, 2].get_xaxis().set_ticks([])\n",
    "ax2[2, 2].get_yaxis().set_ticks([])\n",
    "ax2[2, 2].set_title(\"L2-Map3ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599abb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=3)\n",
    "fig3.set_figheight(15)\n",
    "fig3.set_figwidth(15)\n",
    "ax3[0].imshow(l3_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[0].get_xaxis().set_ticks([])\n",
    "ax3[0].get_yaxis().set_ticks([])\n",
    "ax3[0].set_title(\"L3-Map1\")\n",
    "\n",
    "ax3[1].imshow(l3_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[1].get_xaxis().set_ticks([])\n",
    "ax3[1].get_yaxis().set_ticks([])\n",
    "ax3[1].set_title(\"L3-Map1ReLU\")\n",
    "\n",
    "ax3[2].imshow(l3_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[2].get_xaxis().set_ticks([])\n",
    "ax3[2].get_yaxis().set_ticks([])\n",
    "ax3[2].set_title(\"L3-Map1ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b560fc",
   "metadata": {},
   "source": [
    "We can see that at progressively higher layers of the network, we get coarser representations of the input. Since the filters at the later layers are random, they are not very structured, so we get a kind of blurring effect. These visualizations would be more meaningful in model with learned filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f9f20",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch\n",
    "\n",
    "Now we'll do a more complete CNN example using PyTorch. We'll use the MNIST digits again. The example is based on\n",
    "[Anand Saha's PyTorch tutorial](https://github.com/anandsaha/deep.learning.with.pytorch).\n",
    "\n",
    "PyTorch has a few useful modules for us:\n",
    "1. cuda: GPU-based tensor computations\n",
    "2. nn: Neural network layer implementations and backpropagation via autograd\n",
    "3. torchvision: datasets, models, and image transformations for computer vision problems.\n",
    "\n",
    "torchvision itself includes several useful elements:\n",
    "1. datasets: Datasets are subclasses of torch.utils.data.Dataset. Some of the common datasets available are \"MNIST,\" \"COCO,\" and \"CIFAR.\" In this example we will see how to load MNIST dataset using a custom subclass of the datasets class.\n",
    "2. transforms - Transforms are used for image transformations. The MNIST dataset from torchvision is in PIL image. \n",
    "To convert MNIST images to tensors, we will use `transforms.ToTensor()`.\n",
    "\n",
    "### Begin your pytorch\n",
    "\n",
    "First of all, import pytorch libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# The functional module contains helper functions for defining neural network layers as simple functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f93c4",
   "metadata": {},
   "source": [
    "### Important libraries\n",
    "\n",
    "- torch - main package of pytorch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix–vector multiplication, matrix–matrix multiplication and matrix product.\n",
    "- torchvision - part of pytorch. The torchvision package consists of\n",
    "    - popular datasets\n",
    "    - model architectures\n",
    "    - common image transformations for computer vision\n",
    "\n",
    "### Load the MNIST data\n",
    "\n",
    "Next, let's load the data and transfrom the input elements (pixels) using torchvision, and do transformation to torch type and normalize the data using their mean over the entire training dataset is 0 and its standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired mean and standard deviation\n",
    "mean = 0.0\n",
    "stddev = 1.0\n",
    "\n",
    "# Transform input image\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((mean), (stddev))])\n",
    "\n",
    "# Download dataset to ./data\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "# for colab user, use root_path\n",
    "# mnist_train = datasets.MNIST(root_path + 'data', train=True, download=True, transform=transform)\n",
    "# mnist_valid = datasets.MNIST(root_path + 'data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac69ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67412ed42a00301071a42f56a709eb60",
     "grade": false,
     "grade_id": "cell-b5df4588b523cf11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Show some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mnist_train[12][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[12][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "label = mnist_train[100][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[100][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f25ff",
   "metadata": {},
   "source": [
    "Create **loader** that can connect to the dataset which you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6682e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size if you get out-of-memory error\n",
    "batch_size = 1024\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c61881",
   "metadata": {},
   "source": [
    "## Create CNN model\n",
    "\n",
    "We use 2 convolutional layers followed by 2 fully connected layers. The input size of each image is (28,28,1). We will use stide of size 1 and padding of size 0.\n",
    "\n",
    "For first convolution layer we will apply 20 filters of size (5,5).   CNN output formula $$\\text{output size} = \\frac{W - F + 2P}{S} + 1$$ where $W$ - input, $F$ - filter size, $P$ - padding size and $S$ - stride size.\n",
    "\n",
    "We get $\\frac{(28,28,1) - (5,5,1) + (2*0)}{1} + 1$ for each filter, so for 10 filters we get output size of (24,24,10).\n",
    "\n",
    "The ReLU activation function is applied to the output of the first convolutional layer.\n",
    "\n",
    "For the second convolutional layer, we apply 20 filters of size (5,5), giving us output of size of (20,20,20). Maxpooling with a size of 2 is applied to the output of the second convolutional layer, thereby giving us an output size of of (10,10,20). The ReLU activation function is applied to the output of the maxpooling layer.\n",
    "\n",
    "Next we have two fully connected layers. The input of the first fully connected layer is flattened output of $10*10*20 = 2000$, with 50 nodes. The second layer is the output layer and has 10 nodes.\n",
    "\n",
    "### Important words (for PyTorch)\n",
    "\n",
    "**Tensor** - any matrix arrays which use for calculation, you can call input, output, weight as any tensors. In CNNs, we use \"input tensor\" as input; i.e. image, and \"output tensor\" as output.\n",
    "\n",
    "**Kernel** - filter tensor, or weight tensor. In computer vision, we might call mask tensor or mask matrix.\n",
    "\n",
    "**Channel** - number of depth in tensor, so sometime we call **depth**.\n",
    "\n",
    "**Feature** - Specific characteristic information for using in **dense layers** or **fully connect layers**.\n",
    "\n",
    "**Feature extraction** - the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set.\n",
    "\n",
    "**Stride** - The jump necessary to go from one element to the next one in the specified dimension dim . A tuple of all strides is returned when no argument is passed in.\n",
    "\n",
    "**Padding** - the zero array extends in both sides of tensor.\n",
    "\n",
    "\n",
    "In PyTorch, the function of CNNs is no need to input size, but it needs to fill number of channels and kernel size, including operation in the layer. For dense layer or fully layer, we need to set input features number and output features number. Thus, it is necessary to understand how to calculate tensors and features size in each layer.\n",
    "\n",
    "### PyTorch model architecture\n",
    "\n",
    "PyTorch deep learning models come in (at least) two possible styles:\n",
    "\n",
    "<img src=\"img/NNinPytorch.png\" style=\"width: 500px;\" />\n",
    "\n",
    "1. The PyTorch Sequential API is very expressive when we have a straightforward sequence of operations to perform on the input.\n",
    "2. The PyTorch Module allows more flexible transformations of inputs, combination of multiple inputs, generation of multiple outputs, and so on.\n",
    "\n",
    "### Number of parameters and output tensors size calculation\n",
    "\n",
    "#### CNN parameters\n",
    "\n",
    "Kernel size $k$ in 1 layer for 1 channel output can be calculated by\n",
    "\n",
    "$$k = k_w \\times k_h \\times i_c$$\n",
    "\n",
    "when $k_w$ is width of kernel, $k_h$ is width of kernel, and $i_c$ is input channels. We need to have $o_c$ kernels for release $o_c$ output channels. Therefore, for 1 layer of CNN, number of parameters can be calculated as\n",
    "\n",
    "$$n_p = k \\times o_c = (k_w \\times k_h \\times i_c) \\times o_c$$\n",
    "\n",
    "For bias in CNNs, it usually become all zeros, but we can assign bias CNNs in PyTorch. The **bias size** is equal to the **output tensor size**.\n",
    "\n",
    "#### Fully connect parameters\n",
    "\n",
    "Weight size $s_w$ in 1 layers can be calculated by\n",
    "\n",
    "$$s_w = i_f \\times o_f$$\n",
    "\n",
    "when $i_f$ is input features, and $o_f$ is output features. For bias, the size is equal to **output feature size**.\n",
    "\n",
    "We can calculate that the total parameters number is the parameters of all layers, so the network size can be used from the parameters number. It is useful to tell how efficient of the network (How fast)\n",
    "\n",
    "#### output tensors size\n",
    "\n",
    "If we have an input tensor or image input size $w \\times h$ which want to convolution with $k_w \\times k_h$ kernel size with padding $p$ and stride $s$, we can calculate output tensor size as:\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor \\times \\lfloor \\frac{h+2p-k_h}{s} + 1 \\rfloor $$\n",
    "\n",
    "For example, input image in the first layer is $224 \\times 224$. Using $11 \\times 11$ of kernel size with padding $2$ and stride 4. We calculate\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor = \\lfloor \\frac{224+2(2)-11}{4} + 1 \\rfloor = \\lfloor 55.25 \\rfloor = 55$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc232884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
    "        self.relu1 = nn.ReLU()                            # Same as above\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)     # 20 x 20 x 20  (after 2nd convolution)\n",
    "        #self.conv2_drop = nn.Dropout2d(p=0.5)            # Dropout is a regularization technqiue we discussed in class\n",
    "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 20  (after pooling)\n",
    "        self.relu2 = nn.ReLU()                            # Same as above \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolution Layer 1                    \n",
    "        x = self.conv1(x)                        \n",
    "        x = self.relu1(x)                        \n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)               \n",
    "        #x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Switch from activation maps to vectors\n",
    "        x = x.view(-1, 2000)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af238f",
   "metadata": {},
   "source": [
    "#### Create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "net = CNN_Model()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d93db3",
   "metadata": {},
   "source": [
    "#### Print out the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0fc28",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0885738",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct.cpu() / float(len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    net.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffe37f",
   "metadata": {},
   "source": [
    "We can see that the model is still learning something. We might want to train another 10 epochs or so to see if validation accuracy increases further. For now, though, we'll just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84736f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net.state_dict(), \"lab02/3.model.pth\")\n",
    "# for colab user\n",
    "#torch.save(net.state_dict(), root_path + \"lab02/3.model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(valid_loss, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_accuracy, label='training accuracy')\n",
    "plt.plot(valid_accuracy, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31385c54",
   "metadata": {},
   "source": [
    "What can you conclude from the loss and accuracy curves?\n",
    "1. We are not overfitting (at least not yet)\n",
    "2. We should continue training, as validation loss is still improving\n",
    "3. Validation accuracy is much higher than last week's fully connected models\n",
    "\n",
    "Now let's test on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 9\n",
    "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
    "img = Variable(img)\n",
    "label = mnist_valid[image_index][1]\n",
    "plt.imshow(img[0,0])\n",
    "net.eval()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    net = net.cpu()\n",
    "    img = img.cpu()\n",
    "    \n",
    "output = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00344a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output.data, 1)\n",
    "print(\"Predicted label:\", predicted[0].item())\n",
    "print(\"Actual label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f09253",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc8111ab815b4e5c34ce8b090e22f480",
     "grade": false,
     "grade_id": "cell-adfd720a7dd9d914",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Create your **New** network that can train and test CIFAR-10 dataset. The network that you create must have at least 3 layers. You **Must** do 2 kinds of image size $28\\times28$ and $56\\times56$.\n",
    "\n",
    "1. Design your network and print out your network. (at least 3 layers per network required)\n",
    "2. Design your learning rate\n",
    "3. Show your loss and accuracy of train and test dataset\n",
    "4. Test some data and show the results **at least 10 images**\n",
    "5. You **Must** do 2 kinds of image size $28\\times28$ and $56\\times56$.\n",
    "\n",
    "Note: Each image size for training and testing has 50 points.\n",
    "\n",
    "**Hint**: the dataset can be download as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up preprocessing of CIFAR-10 images to 3x28x28 with normalization\n",
    "# using the magic ImageNet means and standard deviations. You can try\n",
    "# RandomCrop, RandomHorizontalFlip, etc. during training to obtain\n",
    "# slightly better generalization.\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=preprocess)\n",
    "\n",
    "# Split the training set into training and validation sets randomly.\n",
    "# CIFAR-10 train contains 50,000 examples, so let's split 80%/20%.\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "# Download the test set. If you use data augmentation transforms for the training set,\n",
    "# you'll want to use a different transformer here.\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=preprocess)\n",
    "\n",
    "# Dataset objects are mainly designed for datasets that can't fit entirely into memory.\n",
    "# Dataset objects don't load examples into memory until their __getitem__() method is\n",
    "# called. For supervised learning datasets, __getitem__() normally returns a 2-tuple\n",
    "# on each call. To make a Dataset object like this useful, we use a DataLoader object\n",
    "# to optionally shuffle then batch the examples in each dataset. During training.\n",
    "# To keep our memory utilization small, we'll use 4 images per batch, but we could use\n",
    "# a much larger batch size on a dedicated GPU. To obtain optimal usage of the GPU, we\n",
    "# would like to load the examples for the next batch while the current batch is being\n",
    "# used for training. DataLoader handles this by spawining \"worker\" threads that proactively\n",
    "# fetch the next batch in the background, enabling parallel training on the GPU and data\n",
    "# loading/transforming/augmenting on the CPU. Here we use num_workers=2 (the default)\n",
    "# so that two batches are always ready or being prepared.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4,\n",
    "                                              shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39635d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
