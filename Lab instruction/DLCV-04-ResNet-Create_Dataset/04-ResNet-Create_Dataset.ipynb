{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: ResNet and Create Dataset\n",
    "\n",
    "In this lab, we will develop ResNets and SE Nets.\n",
    "\n",
    "## Preliminaries (datasets and data loaders)\n",
    "\n",
    "Let's follow the same process as in lab 2. We import necessary libraries, load images, and transform them to 3x224x224.\n",
    "This version is changed a bit to allow different transforms for the training set (augmentation) and val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "eval_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets.\n",
    "# The copy of the training dataset after the split allows us to keep\n",
    "# the same training/validation split of the original training set but\n",
    "# apply different transforms to the training set and validation set.\n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                  download=True)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = train_preprocess\n",
    "val_dataset.dataset.transform = eval_preprocess\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=eval_preprocess)\n",
    "\n",
    "# DataLoaders for the three datasets\n",
    "\n",
    "BATCH_SIZE=128\n",
    "NUM_WORKERS=4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "ResNet or Residual network has been first publish in [the ResNet paper](https://arxiv.org/pdf/1512.03385.pdf). In the past, there are many problems on the very deep network. Because When the output from convolutional layer come out, the details inside has been changed and cannot guess that what is the input came from. This such problem called **Vanishing gradient**.\n",
    "\n",
    "To solve the vanishing gradient, the paper present about creating shortcut in the system layers.\n",
    "\n",
    "#### ResNet structure\n",
    "\n",
    "The ResNet structure has 4 stages. Each stage combine with number of residual blocks. Thus, in general, we say by show in matrix of 4 digits $[s_1, s_2, s_3, s_4]$. For example if we want to say the structure inside ResNet34, we can write as $[3,4,6,3]$.\n",
    "\n",
    "### ResNet18\n",
    "\n",
    "On page 5 of [the ResNet paper](https://arxiv.org/pdf/1512.03385.pdf), the simplest ResNet\n",
    "described is now known as ResNet18. It is a 1.8 GFLOP CNN with $[2,2,2,2]= 8$ residual blocks (two convolutional\n",
    "layers in each residual block), which along with the initial convolution (7x7 in the paper, 3x3 here)\n",
    "and the final linear / softmax layer gives us 18 layers:\n",
    "\n",
    "<img src=\"img/Resnet18_new.png\" title=\"ResNet18\" style=\"width: 1400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual blocks\n",
    "\n",
    "The residual block is a reusable block.\n",
    "ResNet18 and ResNet34 use very basic residual blocks, but\n",
    "ResNet50, ResNet101, and ResNet152 use more complicated residual blocks\n",
    "with three convolutions, the middle of which is a\n",
    "bottleneck that increases the representational power of the block\n",
    "without an enormous increase in the number of parameters.\n",
    "\n",
    "We need two types of residual block, one that preserves feature map size and one\n",
    "that allows changes to the feature map size:\n",
    "\n",
    "### Please select one\n",
    "\n",
    "<img src=\"img/residualblock_new1.png\" title=\"Residual block\" style=\"width: 800px;\" />\n",
    "\n",
    "Note that only the shape-preserving residual block has a real identity mapping.\n",
    "The 1x1 strided convolution is the simplest way to allow changes in the input\n",
    "feature map size, but since the parameters are learned, after training, the result\n",
    "may be quite different from an identity mapping.\n",
    "\n",
    "Let's see how to implement a residual block in a resuable way. This\n",
    "code is modified from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    BasicBlock: Simple residual block with two conv layers\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck blocks\n",
    "\n",
    "When we use Residual block more than 50 layers of CNN. The residual network will be changed to be bottleneck block, whick add another 1x1 CNN layer inside the residual network. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. \n",
    "\n",
    "Here's the bottlneck version with three layers per residual block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    '''\n",
    "    BottleneckBlock: More powerful residual block with three convs, used for Resnet50 and up\n",
    "    '''\n",
    "    EXPANSION = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.EXPANSION * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If the output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != self.EXPANSION * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.EXPANSION * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "\n",
    "Here is the whole shebang for ResNet, with the layer sizes tailored a bit to our input size of 64x64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # FC layer = 1 layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.EXPANSION, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.EXPANSION\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this template we can make various ResNets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with two sets of two convolutions each: 2*2 + 2*2 + 2*2 + 2*2 = 16 conv layers\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+16+1 = 18\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def ResNet34(num_classes):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of two convolutions each: 3*2 + 4*2 + 6*2 + 3*2 = 32\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+32+1 = 34\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet50(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of three convolutions each: 3*3 + 4*3 + 6*3 + 3*3 = 48\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+48+1 = 50\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 23, 3] sets of three convolutions each: 3*3 + 4*3 + 23*3 + 3*3 = 99\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+99+1 = 101\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 8, 36, 3] sets of three convolutions each: 3*3 + 8*3 + 36*3 + 3*3 = 150\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+150+1 = 152\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's test it! Here's a `train_model()` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    '''\n",
    "    train_model: train a model on a dataset\n",
    "    \n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # for process anything, device and dataset must put in the same place.\n",
    "                # If the model is in GPU, input and output must set to GPU\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                # it uses for update training weights\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        # print('outputs', outputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function still be the same as lab3. Then, you can train Resnet function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18().to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(resnet, dataloaders, criterion, optimizer, 25, 'resnet18_bestsofar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and Excite networks\n",
    "\n",
    "Squeeze and Excite networks (SENet) is a building block for CNNs that improves channel interdependencies at almost no computational cost. The modification from the ordinary ResNet is easy. The main idea of SENet is add parameters in each channel, then the network can adaptively adjust the weighting of each feature map.\n",
    "\n",
    "SENets are all about changing this by adding a content aware mechanism to weight each channel adaptively. In it’s most basic form this could mean adding a single parameter to each channel and giving it a linear scalar how relevant each one is.\n",
    "\n",
    "The concept of squeeze and excite (SENet) is shown here:\n",
    "\n",
    "<img src=\"img/SEInceptionModule.PNG\" title=\"se block\" style=\"width: 800px;\" />\n",
    "\n",
    "<img src=\"img/SEResnetModule.PNG\" title=\"se block\" style=\"width: 800px;\" />\n",
    "\n",
    "Implementation is beautifully simple. Here's an example of an SE module\n",
    "from https://github.com/moskomule/senet.pytorch/blob/23839e07525f9f5d39982140fccc8b925fe4dee9/senet/se_module.py#L4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE modules can be added anywhere you like:\n",
    "\n",
    "<img src=\"img/SEblockSet.PNG\" title=\"se add\" style=\"width: 800px;\" />\n",
    "\n",
    "Let's use the standard option (option b above) recommended by the authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSEBasicBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualSEBasicBlock: Standard two-convolution residual block with an SE Module between the\n",
    "                          second convolution and the identity addition\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.se = SELayer(out_planes, reduction)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with a 1x1 conv\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)              # se net add here\n",
    "        out += self.shortcut(x)         # shortcut just plus it!!!\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResSENet18(num_classes = 10):\n",
    "    return ResNet(ResidualSEBasicBlock, [2, 2, 2, 2], num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the SE version of ResNet18 and compare in terms of time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ressenet = ResSENet18().to(device)\n",
    "# Optimizer, loss function\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "params_to_update2 = ressenet.parameters()\n",
    "optimizer2 = optim.Adam(params_to_update2, lr=0.01)\n",
    "\n",
    "best_model2, val_acc_history2, loss_acc_history2 = train_model(ressenet, dataloaders, criterion2, optimizer2, 25, 'ressenet18_bestsofar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(val_acc_history, loss_acc_history, val_acc_history2, loss_acc_history2):\n",
    "    plt.plot(loss_acc_history, label = 'ResNet18')\n",
    "    plt.plot(loss_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Training loss over time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(val_acc_history, label = 'ResNet18')\n",
    "    plt.plot(val_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Validation accuracy over time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(val_acc_history, loss_acc_history, val_acc_history2, loss_acc_history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we can see that the additional parameters accelerate learning of the training set without\n",
    "causing any degredation on the validation set and in fact improving validation set performance early\n",
    "on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own dataset\n",
    "\n",
    "If you want to use the model that you created, downloaded with your own project, you must know that each dataset does not store in the same format. You need to consider the data to get images and label as you want. For computer vision dataset, there are some example types as:\n",
    "\n",
    "1. Classification: images, labels\n",
    "    - folderClassA, folderClassB\n",
    "    - image_name\n",
    "    - images folder, csv_labels\n",
    "2. Detection: images, annotations\n",
    "    - Yolo: images folder, annotation files\n",
    "3. Segmentation: images, annotations\n",
    "    - images folder, masks folder\n",
    "    - images folder, annotation files\n",
    "4. Image synthesis: images, labels (optional)\n",
    "5. Image transfer: imagesA, imagesB\n",
    "\n",
    "In this lab, I will explain only image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Kaprao-Horapa\n",
    "\n",
    "First, let load the [vege_dataset.zip](https://www.dropbox.com/s/eip79rx1mmofbov/vege_dataset.zip?dl=0).\n",
    "The dataset contains 2 classes of kaprao and horapa. Both are basils but different family and using.\n",
    "\n",
    "Extract file and see the folder inside\n",
    "\n",
    "The dataset contains 2 folders which 2 difference name, so we can use the folder as dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset class using pytorch\n",
    "\n",
    "Let's create the empty dataset class. The input of the class are\n",
    "- the dataset library of <code>../vege_dataset/</code>, when <code>..</code> is the root path of your dataset.\n",
    "- transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BasilDataset(Dataset):\n",
    "    def __init__(self, root_path=\"/vege_dataset/\", transform=None):\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important function of the dataset class are\n",
    "- <code>__init__</code>: The initialize dataset, this is used for count all data and collect all paths for each data in your dataset.\n",
    "- <code>__len__</code>: The function return total number of dataset\n",
    "- <code>__getitem__</code>: The function is used for select 1 row of data in dataset and return as input and label as you want. You don't need to care about batch items. The **DataLoader** class will help you load later.\n",
    "\n",
    "### Initialize dataset\n",
    "Then you need to write <code>__init__</code> function to get all path files of your dataset.\n",
    "\n",
    "Then, you can insert <code>len</code> as the total number of dataset you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "class BasilDataset(Dataset):\n",
    "    def __init__(self, root_path=\"/vege_dataset/\", transform=None):\n",
    "        # keep root directory\n",
    "        self.dir = root_path\n",
    "        # keep transform\n",
    "        self.transform = transform\n",
    "\n",
    "        # read all files in kapao and horapa folder\n",
    "        list_kaprao = listdir(root_path + 'kapao/')\n",
    "        list_horapa = listdir(root_path + 'horapa/')\n",
    "        # calculate all number for each class (just in case)\n",
    "        self.kaprao_len = len(list_kaprao)\n",
    "        self.horapa_len = len(list_horapa)\n",
    "\n",
    "        # put the data file path into ids\n",
    "        self.ids = [self.dir + 'kapao/' + file for file in list_kaprao if not file.startswith('.')]\n",
    "        self.ids.extend([self.dir + 'horapa/' + file for file in list_horapa if not file.startswith('.')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.kaprao_len + self.horapa_len\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get one item of your dataset in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "class BasilDataset(Dataset):\n",
    "    def __init__(self, root_path=\"vege_dataset/\", transform=None):\n",
    "        # keep root directory\n",
    "        self.dir = root_path\n",
    "        # keep transform\n",
    "        self.transform = transform\n",
    "\n",
    "        # read all files in kapao and horapa folder\n",
    "        list_kaprao = listdir(root_path + 'kapao/')\n",
    "        list_horapa = listdir(root_path + 'horapa/')\n",
    "        # calculate all number for each class (just in case)\n",
    "        self.kaprao_len = len(list_kaprao)\n",
    "        self.horapa_len = len(list_horapa)\n",
    "\n",
    "        # put the data file path into ids\n",
    "        self.ids = [self.dir + 'kapao/' + file for file in list_kaprao if not file.startswith('.')]\n",
    "        self.ids.extend([self.dir + 'horapa/' + file for file in list_horapa if not file.startswith('.')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.kaprao_len + self.horapa_len\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        img_file = idx\n",
    "        \n",
    "        # open photo\n",
    "        pil_img = Image.open(img_file)\n",
    "        \n",
    "        # resize, normalize and convert to pytorch tensor\n",
    "        if self.transform:\n",
    "            img = self.transform(pil_img)\n",
    "        self.pil_img = pil_img\n",
    "            \n",
    "        # get label from file list counter\n",
    "        if i < self.kaprao_len:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': label,\n",
    "            'file_name' : img_file,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset\n",
    "\n",
    "Now you can test your dataset to get images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"vege_dataset/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(28), # CenterCrop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = BasilDataset(root, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_label = ['kaprao', 'horapa']\n",
    "\n",
    "batch = dataset[0]\n",
    "image, label, filename = batch['image'], batch['label'], batch['file_name']\n",
    "pil_img = Image.open(filename)\n",
    "\n",
    "print(output_label[label])\n",
    "print(filename)\n",
    "# (3, 224, 224) pytorch\n",
    "# pyplot -> (224,224,3)\n",
    "plt.imshow(pil_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use resnet class from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18(2).to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "loss_history = []\n",
    "loss_history_epoch = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for batch in train_loader:\n",
    "        image, label, filename = batch['image'], batch['label'], batch['file_name']\n",
    "            \n",
    "        epoch_iter += image.shape[0]\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # training only\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = resnet(image)\n",
    "\n",
    "        # 0, 1, 0, 0 ---> 0.2, 0.6, 0.1, 0.1\n",
    "        loss = criterion(output, label)       # training\n",
    "\n",
    "        # prediction - real use\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        running_corrects += torch.sum(preds == label.data)\n",
    "\n",
    "        loss.backward()       # back propagation -> calculate that how much value to update weight\n",
    "        optimizer.step()      #update weight\n",
    "\n",
    "        loss_history.append(loss.item() * image.size(0))\n",
    "        if (epoch_iter % 640 == 0):\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(epoch_iter, loss.item(), running_corrects / epoch_iter))\n",
    "        \n",
    "    loss_history_epoch.append(running_loss / epoch_iter)\n",
    "    accuracy.append(running_corrects / epoch_iter)\n",
    "\n",
    "    print('Epoch: {} Loss: {:.4f} Acc: {:.4f}'.format(epoch, running_loss / epoch_iter, running_corrects / epoch_iter * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home exercises\n",
    "1. Run the lab instruction. For the dataset part, separate to be 90% of train set and 10% of test set (random). (30 points)\n",
    "2. Try to create InceptionResNet by your own. Remind that 1 inception block is one ResNet Module. You can use the pattern of InceptionNet from chapter 3. Train the model using CIFAR10 dataset. plot graph and accuracy output. (40 points)\n",
    "3. Find your own dataset which contains at least 3 classes. If you download from somewhere, please reference it. Make your own dataset class, explain how to setup your data and the label. Train the dataset in ResNet and InceptionResNet, show your results. (30 points)\n",
    "\n",
    "### Turn-in report\n",
    "\n",
    "Export the output of the lab in PDF. You can do in the same file or create separate files of your homework and in-class exercise. Submit in PDF file and Jupyter notebook\n",
    "\n",
    "You don't need to upload dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
